# Tokenization-NLP-
Tokenization is one of the first steps of data processing when it comes to working with data in the domain of NLP.<br>

We will use spacy to tokenize input sentences and compare it's results with basic tokenization performed via Python.<br>
##### Requirements
pip install spacy<br>
python -m spacy download en_core_web_sm
