{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vanshikaamahajan/Tokenization-NLP-/blob/main/Tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIRh9Fq3l-3-",
        "outputId": "70dbc991-1b45-47b9-b0a6-301b19e0adba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token 0 - Hi,\n",
            "Token 1 - There\n",
            "Token 2 - !\n",
            "Token 3 - This\n",
            "Token 4 - is\n",
            "Token 5 - a\n",
            "Token 6 - notebook\n",
            "Token 7 - on\n",
            "Token 8 - Tokenization\n"
          ]
        }
      ],
      "source": [
        "# tokenization of a text using python\n",
        "doc = \"Hi, There ! This is a notebook on Tokenization\"\n",
        "for i,token in enumerate(doc.split(\" \")):\n",
        "    print(\"Token {} - {}\".format(i,token))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeXJUd7El-4B"
      },
      "source": [
        "But, This straightforward approach of tokenisation encounters a lot of loopholes as text contains tokens which are noisy. Like associated with hyphens or name of various nouns.\n",
        "\n",
        "* BERT uses the concept of sub-word tokens to permute over various combinations of characters which can form part of the vocabulary. It helps it in narrowing down to the OOV(Out of vocabulary) tokens.\n",
        "\n",
        "Thus, We use spacy as an efficient tokenizer for NLP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhHQ59dnl-4C",
        "outputId": "b178a465-76b9-4f7a-fe3b-9259c524a0bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token: Hi\n",
            "Token: ,\n",
            "Token: There\n",
            "Token: !\n",
            "Token: This\n",
            "Token: is\n",
            "Token: a\n",
            "Token: notebook\n",
            "Token: on\n",
            "Token: Tokenization\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "#tokenizigng\n",
        "doc = nlp(\"Hi, There ! This is a notebook on Tokenization\")\n",
        "for token in doc:\n",
        "    print(\"Token: {}\".format(token))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95I1iS6kl-4E"
      },
      "source": [
        "One can also add his own tokenizer rules. Visit  <a href=\"https://spacy.io/usage/linguistic-features#special-cases\"> Link </a>\n",
        "\n",
        "Besides this, Each model like BERT, BART and its variants come with their own tokenizers. Let's have a look at one such variant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "hFeNh6jhl-4E"
      },
      "outputs": [],
      "source": [
        "import tensorflow\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "sentences = [\n",
        "    \"Hi, This is our first Tokenizer Notebook\",\n",
        "    \"Glad to see you here.\",\n",
        "    \"What are you upto ?\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYgHnB6Bl-4F",
        "outputId": "373f0f76-2326-4e3c-9063-e31a564a2408"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'you': 1, 'hi': 2, 'this': 3, 'is': 4, 'our': 5, 'first': 6, 'tokenizer': 7, 'notebook': 8, 'glad': 9, 'to': 10, 'see': 11, 'here': 12, 'what': 13, 'are': 14, 'upto': 15}\n"
          ]
        }
      ],
      "source": [
        "tokenizer = Tokenizer(num_words=20)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_idx = tokenizer.word_index\n",
        "print(word_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kag4NFHml-4H",
        "outputId": "6b7b5104-f8b5-4241-9394-8e4dea123acb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 3, 4, 5, 6, 7, 8]\n",
            "[9, 10, 11, 1, 12]\n",
            "[13, 14, 1, 15]\n"
          ]
        }
      ],
      "source": [
        "# converting each tokenized sentence into sequence\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "for seq in sequences:\n",
        "    print(seq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmQ--CPcl-4I",
        "outputId": "ac8c5093-6033-4086-b9ca-a23d2b50adbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2 3 4 5 6 7 8]\n",
            "[ 9 10 11  1 12  0  0]\n",
            "[13 14  1 15  0  0  0]\n"
          ]
        }
      ],
      "source": [
        "# to ensure that each sequence contains same number of tokens which are a primary need for any NN. We'll pad\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "padded_sequences = pad_sequences(sequences, padding='post')\n",
        "for seq in padded_sequences:\n",
        "    print(seq)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "dl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11 (default, Jul 27 2021, 14:32:16) \n[GCC 7.5.0]"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "50052c996937e9a0e161d422489677fdaadc23d756ac209b7397e80e5ea8cea0"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}